import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Step 1: Data Generation and Labeling
np.random.seed(0)  # for reproducibility
data = []
labels = []
for _ in range(1000):
//to type of diff lenth <(15) and lenth >50
//save data type diff
    length = np.random.randint(5, 15)  # Random length between 5 and 15
    spread = np.random.randint(1, 50)  # Random spread to influence variance
    list_data = np.random.normal(loc=0, scale=spread, size=length)
    variance = np.var(list_data)
    label = 1 if variance < 50 else 0  # Low variance if < 50, else high variance
    data.append(list_data)
    labels.append(label)


# Step 2: Feature Engineering
features = []
for lst in data:
    mean = np.mean(lst)
    std_dev = np.std(lst)
    rng = np.max(lst) - np.min(lst)
    features.append([mean, std_dev, rng])

features_df = pd.DataFrame(features, columns=['Mean', 'StdDev', 'Range'])

//determin two diff type using K-means clustring and giving mean to each type
# Step 3: Clustering with K-means
kmeans = KMeans(n_clusters=2, random_state=0)
kmeans.fit(features_df)
cluster_labels = kmeans.labels_

//test result using logisticregression by splitting the two param(rang(diff from max and min),stdDiv(standard diviation (correct part of part taking (cercle(cercle)))))
# Step 4: Model Training and Evaluation
X_train, X_test, y_train, y_test = train_test_split(features_df, cluster_labels, test_size=0.2, random_state=0)
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print(f"Accuracy: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)

# Plotting the results
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.scatter(X_test['StdDev'], X_test['Range'], c=y_test, cmap='coolwarm', label='Actual Label')
plt.xlabel('Standard Deviation')
plt.ylabel('Range')
plt.title('True Labels')
plt.colorbar()

plt.subplot(1, 2, 2)
plt.scatter(X_test['StdDev'], X_test['Range'], c=y_pred, cmap='coolwarm', label='Predicted Label')
plt.xlabel('Standard Deviation')
plt.ylabel('Range')
plt.title('Predicted Labels')
plt.colorbar()
plt.tight_layout()
plt.show()
!pip install flask-ngrok

from flask_ngrok import run_with_ngrok
from flask import Flask, request, jsonify
import threading

app = Flask(__name__)

def predict_variance(new_list, model):
    # Calculate the features of the new list
    mean = np.mean(new_list)
    std_dev = np.std(new_list)
    rng = np.max(new_list) - np.min(new_list)
    features = np.array([[mean, std_dev, rng]])

    # Predict using the logistic regression model
    prediction = model.predict(features)
    return "1" if prediction == 1 else "0"



@app.route('/predict', methods=['POST'])
def predict():
    data = request.json['data']  # Assuming data is sent as JSON

    # Extract the single number and the list of numbers from the received data
    result = predict_variance(data, model)

    return jsonify({'feedback': result})

if __name__ == "__main__":
    threading.Thread(target=app.run).start()

